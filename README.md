# weiyi.py
# -*- coding:UTF-8 -*- import requests import re from bs4 import BeautifulSoup import os from urllib.request import urlretrieve from urllib import error import urllib.error import time if __name__=='__main__':     gg_list=[]     page_list=[]     url_5=input('输入首页地址:')     gg_list=url_5     page_3=int(input('页数:'))+1     name=input('请输入所要存放或创建的文件夹:')     list_name1=[]     headers={"User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}     for page in range(1,page_3):         if page==1:             page_list.append(url_5)         else:             k=gg_list[:-10]             page_list.append(k+'%d.html'%page)     for url_zu in page_list:         print(url_zu)         res=requests.get(url=url_zu,headers=headers)         res.encoding='gb2312'         html=res.text         bf_1=BeautifulSoup(html,'lxml')         bf_2=bf_1.find_all(class_='ABox')         list_url1=[]         for each_1 in bf_2:              list_url1.append(each_1.a.get('href')+'='+each_1.img.get('alt'))         for each_2 in list_url1:             info=each_2.split('=')             list_url1=info[0]             list_url=[]             list_filename=[]             g=1             c_url=list_url1             w_name=str(name)             #x=50             a=c_url[:-5]             res=requests.get(url=c_url,headers=headers)             res.encoding='gb2312'             html=res.text             bf_3=BeautifulSoup(html,'lxml')             ccg=str(bf_3.find_all(class_='totalpage'))             gg=re.findall(r'\d+',ccg)             x=int(gg[0])+1             for num in range(1,x):                 if num==1:                     list_url.append(a+'.html')                 else:                     list_url.append(a+'_%d.html'%num)                  for each_img in list_url:                 img_info=each_img.split()                 c=0                 headers={"User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"}                 try:                     req=requests.get(url=img_info[0],headers=headers)                     c=1                 except urllib.error.HTTPError as e:                     print(e.code)                 except urllib.error.URLError as e:                     print(e.reason)                 if c==1:                     biao='g'                     q=0                     p=0                     req.encoding='gb2312'                     html=req.text                     bf=BeautifulSoup(html,'lxml')                     scr_url=bf.find_all(class_='big-pic')                     for ee in scr_url:                         try:                             filename=str(ee.img.get('alt'))+'%d.jpg'%g                             z_url=ee.img.get('src')                             q=1                         except:                             p+=1                     if q==1:                                 req1=requests.get(z_url)                         try:                             req1.encoding='gb2312'                             cc=req1.text                             bf_2=BeautifulSoup(cc,'lxml')                             biao=str(bf_2.title)                         except error.HTTPError as e:                             print('gg')                         g+=1                             print('下载:%s'%filename)                         if  '%s'%w_name not in os.listdir():                             os.makedirs('%s'%w_name)                         if biao !='&lt;title>404 Not Found&lt;/title>':                             urlretrieve(url = z_url,filename ='%s/'%w_name +filename)                         time.sleep(0.25)     print(p)                        print('下载完成')
